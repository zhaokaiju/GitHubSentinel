import json
import requests
from openai import OpenAI  # å¯¼å…¥OpenAIåº“ç”¨äºè®¿é—®GPTæ¨¡å‹
from logger import LOG  # å¯¼å…¥æ—¥å¿—æ¨¡å—

class LLM:
    def __init__(self, config):
        """
        åˆå§‹åŒ– LLM ç±»ï¼Œæ ¹æ®é…ç½®é€‰æ‹©ä½¿ç”¨çš„æ¨¡å‹ï¼ˆOpenAI æˆ– Ollamaï¼‰ã€‚

        :param config: é…ç½®å¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰çš„æ¨¡å‹é…ç½®å‚æ•°ã€‚
        """
        self.config = config
        self.model = config.llm_model_type.lower()  # è·å–æ¨¡å‹ç±»å‹å¹¶è½¬æ¢ä¸ºå°å†™
        if self.model == "openai":
            self.client = OpenAI()  # åˆ›å»ºOpenAIå®¢æˆ·ç«¯å®ä¾‹
        elif self.model == "ollama":
            self.api_url = config.ollama_api_url  # è®¾ç½®Ollama APIçš„URL
        else:
            LOG.error(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model}")
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model}")  # å¦‚æœæ¨¡å‹ç±»å‹ä¸æ”¯æŒï¼ŒæŠ›å‡ºé”™è¯¯

    def generate_report(self, system_prompt, user_content):
        """
        ç”ŸæˆæŠ¥å‘Šï¼Œæ ¹æ®é…ç½®é€‰æ‹©ä¸åŒçš„æ¨¡å‹æ¥å¤„ç†è¯·æ±‚ã€‚

        :param system_prompt: ç³»ç»Ÿæç¤ºä¿¡æ¯ï¼ŒåŒ…å«ä¸Šä¸‹æ–‡å’Œè§„åˆ™ã€‚
        :param user_content: ç”¨æˆ·æä¾›çš„å†…å®¹ï¼Œé€šå¸¸æ˜¯Markdownæ ¼å¼çš„æ–‡æœ¬ã€‚
        :return: ç”Ÿæˆçš„æŠ¥å‘Šå†…å®¹ã€‚
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_content},
        ]

        # æ ¹æ®é€‰æ‹©çš„æ¨¡å‹è°ƒç”¨ç›¸åº”çš„ç”ŸæˆæŠ¥å‘Šæ–¹æ³•
        if self.model == "openai":
            return self._generate_report_openai(messages)
        elif self.model == "ollama":
            return self._generate_report_ollama(messages)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model}")

    def _generate_report_openai(self, messages):
        """
        ä½¿ç”¨ OpenAI GPT æ¨¡å‹ç”ŸæˆæŠ¥å‘Šã€‚

        :param messages: åŒ…å«ç³»ç»Ÿæç¤ºå’Œç”¨æˆ·å†…å®¹çš„æ¶ˆæ¯åˆ—è¡¨ã€‚
        :return: ç”Ÿæˆçš„æŠ¥å‘Šå†…å®¹ã€‚
        """
        LOG.info(f"ä½¿ç”¨ OpenAI {self.config.openai_model_name} æ¨¡å‹ç”ŸæˆæŠ¥å‘Šã€‚")
        try:
            response = self.client.chat.completions.create(
                model=self.config.openai_model_name,  # ä½¿ç”¨é…ç½®ä¸­çš„OpenAIæ¨¡å‹åç§°
                messages=messages
            )
            LOG.debug("GPT å“åº”: {}", response)
            return response.choices[0].message.content  # è¿”å›ç”Ÿæˆçš„æŠ¥å‘Šå†…å®¹
        except Exception as e:
            LOG.error(f"ç”ŸæˆæŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
            raise

    def _generate_report_ollama(self, messages):
        """
        ä½¿ç”¨ Ollama LLaMA æ¨¡å‹ç”ŸæˆæŠ¥å‘Šã€‚

        :param messages: åŒ…å«ç³»ç»Ÿæç¤ºå’Œç”¨æˆ·å†…å®¹çš„æ¶ˆæ¯åˆ—è¡¨ã€‚
        :return: ç”Ÿæˆçš„æŠ¥å‘Šå†…å®¹ã€‚
        """
        LOG.info(f"ä½¿ç”¨ Ollama {self.config.ollama_model_name} æ¨¡å‹ç”ŸæˆæŠ¥å‘Šã€‚")
        try:
            payload = {
                "model": self.config.ollama_model_name,  # ä½¿ç”¨é…ç½®ä¸­çš„Ollamaæ¨¡å‹åç§°
                "messages": messages,
                "max_tokens": 4000,
                "temperature": 0.7,
                "stream": False
            }

            response = requests.post(self.api_url, json=payload)  # å‘é€POSTè¯·æ±‚åˆ°Ollama API
            response_data = response.json()

            # è°ƒè¯•è¾“å‡ºæŸ¥çœ‹å®Œæ•´çš„å“åº”ç»“æ„
            LOG.debug("Ollama å“åº”: {}", response_data)

            # ç›´æ¥ä»å“åº”æ•°æ®ä¸­è·å– content
            message_content = response_data.get("message", {}).get("content", None)
            if message_content:
                return message_content  # è¿”å›ç”Ÿæˆçš„æŠ¥å‘Šå†…å®¹
            else:
                LOG.error("æ— æ³•ä»å“åº”ä¸­æå–æŠ¥å‘Šå†…å®¹ã€‚")
                raise ValueError("Ollama API è¿”å›çš„å“åº”ç»“æ„æ— æ•ˆ")
        except Exception as e:
            LOG.error(f"ç”ŸæˆæŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
            raise

if __name__ == '__main__':
    from config import Config  # å¯¼å…¥é…ç½®ç®¡ç†ç±»
    config = Config()
    llm = LLM(config)

    markdown_content="""
# Progress for langchain-ai/langchain (2024-08-20 to 2024-08-21)

## Issues Closed in the Last 1 Days
- partners/chroma: release 0.1.3 #25599
- docs: few-shot conceptual guide #25596
- docs: update examples in api ref #25589
"""

    # ç¤ºä¾‹ï¼šç”Ÿæˆ GitHub æŠ¥å‘Š
    system_prompt = """
    # Role
    You are a professional GitHub report generation assistant, capable of creating detailed, accurate, and well-organized GitHub reports.

    ## Skills
    ### Skill 1: Generate GitHub Reports
    1. When the user requests a GitHub report, first ask about the specific scope the report needs to cover, such as specific repositories or time periods. Skip this step if the user has already provided this information.
    2. Analyze and organize relevant GitHub data based on the detailed information provided by the user.
    3. Generate a comprehensive GitHub report according to the analysis results. The report should include but not be limited to aspects such as repository activity, code contribution status, and issue handling progress.
    === Reply Example ===
    ### GitHub Report
    - **Repository Name**: <Specific repository name>
    - **Report Time Period**: <Start time>-<End time>
    - **Repository Activity**: During this period, the repository had <X> commits, <X> pushes, etc., with specific data descriptions.
    - **Code Contribution Status**: <List in detail the main contributors and the number of lines of code contributed, etc.>
    - **Issue Handling Progress**: A total of <X> issues were created, <X> were resolved, and <X> remain unresolved, etc., with detailed information.
    === End of Example ===

    ## Constraints:
    - Only answer questions related to generating GitHub reports and reject irrelevant topics.
    - The content of the GitHub report output must be clearly structured and logically coherent, organized according to the given format framework requirements.
    - The data in the report must be accurate and true, based on effective analysis of GitHub data. 

    ## Complete High-quality Prompt Example
    Original Prompt: â€œMovie narrator who can introduce the latest moviesâ€
    Complete High-quality Prompt:
    # Role
    You are a sharp movie narrator who can use sharp and humorous language to explain movie plots to users, introduce the latest released movies, and explain movie-related knowledge in language that ordinary people can understand.

    ## Skills
    ### Skill 1: Recommend the Latest Released Movies
    1. When the user asks you to recommend the latest movies, you need to first understand what type of movies the user likes. Skip this step if you already know.
    2. If you don't know the movie the user mentioned, use the tool to search for the movie and understand its genre.
    3. Based on the user's movie preferences, recommend several movies that are currently showing and upcoming.
    === Reply Example ===
    - ğŸ¬ Movie Name: <Movie name>
    - ğŸ• Release Time: <Release date of the movie in mainland China>
    - ğŸ’¡ Movie Introduction: <Summarize the plot of the movie in 100 words>
    === End of Example ===

    ### Skill 2: Introduce a Movie
    1. When the user asks you to introduce a certain movie, use the tool to search for links to movie introductions.
    2. If the information obtained at this time is not comprehensive enough, continue to use the tool to open relevant links in the search results to understand the details of the movie.
    3. Generate a movie introduction based on the search and browsing results.

    ### Skill 3: Introduce Movie Concepts
    - You can use the knowledge in the dataset, call the knowledge base to search for relevant knowledge, and introduce basic concepts to the user.
    - Use a movie familiar to the user to give a practical scenario to explain the concept.

    ## Constraints:
    - Only discuss topics related to movies and reject topics unrelated to movies.
    - The output content must be organized according to the given format and cannot deviate from the framework requirements.
    - The summary part cannot exceed 100 words.
    - Only output content already in the knowledge base. For books not in the knowledge base, use the tool to understand.
    - Please use Markdown's ^^ to indicate the source of reference.
    """
    github_report = llm.generate_report(system_prompt, markdown_content)
    LOG.debug(github_report)
